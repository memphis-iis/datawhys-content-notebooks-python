{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression  \n",
    "\n",
    "In many data sets there may be several predictor variables that have an effect on a response variable.\n",
    " In fact, the *interaction* between variables may also be used to predict response.\n",
    " When we incorporate these additional predictor variables into the analysis the model is called *multiple regression* .\n",
    " The multiple regression model builds on the simple linear regression model by adding additional predictors with corresponding parameters.\n",
    "\n",
    "## Multiple Regression Model\n",
    "Let's suppose we are interested in determining what factors might influence a baby's birth weight.\n",
    " In our data set we have information on birth weight, our response, and predictors: motherâ€™s age, weight and height and gestation period.\n",
    " A *main effects model*  includes each of the possible predictors but no interactions.\n",
    " Suppose we name these features as in the chart below.\n",
    " \n",
    "| Variable | Description       |\n",
    "|----------|:-------------------|\n",
    "| BW       | baby birth weight |\n",
    "| MA       | mother's age      |\n",
    "| MW       | mother's weight   |\n",
    "| MH       | mother's height   |\n",
    "| GP       | gestation period  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the theoretical main effects multiple regression model is \n",
    "\n",
    "$$BW = \\beta_0 + \\beta_1 MA + \\beta_2 MW + \\beta_3 MH + \\beta_4 GP+ \\epsilon.$$ \n",
    "\n",
    "Now we have five parameters to estimate from the data, $\\beta_0, \\beta_1, \\beta_2, \\beta_3$ and $\\beta_4$.\n",
    " The random error term, $\\epsilon$ has the same interpretation as in simple linear regression and is assumed to come from a normal distribution with mean equal to zero and variance equal to $\\sigma^2$.\n",
    " Note that multiple regression also includes the polynomial models discussed in the simple linear regression notebook.\n",
    " \n",
    "One of the most important things to notice about the equation above is that each variable makes a contribution **independently** of the other variables.\n",
    "This is sometimes called **additivity**: the effects of predictor variable are added together to get the total effect on `BW`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Effects\n",
    "\n",
    "Suppose in the example, through exploratory data analysis, we discover that younger mothers with long gestational times tend to have heavier babies, but older mother with short gestational times tend to have lighter babies.\n",
    " This could indicate an interaction effect on the response.\n",
    " When there is an interaction effect, the effects of the variables involved are not additive.\n",
    " \n",
    " Different numbers of variables can be involved in an interaction.\n",
    " When two features are involved in the interaction it is called a *two-way interaction* .\n",
    " There are three-way and higher interactions possible as well, but they are less common in practice.\n",
    " The *full model*  includes main effects and all interactions.\n",
    " For the example given here there are 6 two-way interactions possible between the variables, 4 possible three-way, and 1 four-way interaction in the full model.\n",
    " \n",
    " Often in practice we fit the full model to check for significant interaction effects.\n",
    " If there are no interactions that are significantly different from zero, we can drop the interaction terms and fit the main effects model to see which of those effects are significant.\n",
    " If interaction effects are significant (important in predicting the behavior of the response) then we will interpret the effects of the model in terms of the interaction.\n",
    " \n",
    "<!--  NOTE: not sure if correction for multiple comparisons is outside the scope here; I would in general not recommend to students that they test all possible interactions unless they had a theoretical reason to, or unless they were doinging something exploratory and then would collect new data to test any interaction found. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Suppose we run a full model for the four variables in our example and none of the interaction terms are significant.\n",
    " We then run a main effects model and we get parameter estimates as shown in the table below.\n",
    " \n",
    "| Coefficients | Estimate | Std. Error | p-value |\n",
    "|--------------|----------|------------|---------|\n",
    "| Intercept    | 36.69    | 5.97       | 1.44e-6 |\n",
    "| MA           | 0.36     | 1.00       | 0.7197  |\n",
    "| MW           | 3.02     | 0.85       | 0.0014  |\n",
    "| MH           | -0.02    | 0.01       | 0.1792  |\n",
    "| GP           | -0.81    | 0.66       | 0.2311  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the p-value is the probability of getting the estimate that we got from the data or something more extreme (further from zero).\n",
    " Small p-values (typically less than 0.05) indicate the associated parameter is different from zero, implying that the associated covariate is important to predict response.\n",
    " In our birth weight example, we see the p-value for the intercept is very low $1.44 \\times 10^{-6}$ and so the intercept is not at zero.\n",
    " The mother's weight (MW) has p-value 0.0014 which is very small, indicating that mother's weight has an important (significant) impact on her baby's birth weight.\n",
    " The p-value from all other Wald tests are large: 0.7197, 0.1792, and 0.2311, so we know none of these variables are important when predicting the birth weight.\n",
    " \n",
    "  We can modify the coefficient of determination to account for having more than one predictor in the model, called the *adjusted R-square* .\n",
    " R-square has the property that as you add more terms, it will always increase.\n",
    " The adjustment for more terms takes this into consideration.\n",
    " For this data the adjusted R-square is 0.8208, indicating a reasonably good fit.\n",
    "\n",
    "  Different combinations of the variables included in the model may give better or worse fits to the data.\n",
    " We can use several methods to select the \"best\" model for the data.\n",
    " One example is called *forward selection* .\n",
    " This method begins with an empty model (intercept only) and adds variables to the model one by one until the full main effects model is reached.\n",
    " In each forward step, you add the one variable that gives the best improvement to the fit.\n",
    " There is also *backward selection*  where you start with the full model and then drop the least important variables one at a time until you are left with the intercept only.\n",
    " If there are not too many features, you can also look at all possible models.\n",
    " Typically these models are compared using the AIC (Akaike information criterion) which measures the relative quality of models.\n",
    " Given a set of models, the preferred model is the one with the minimum AIC value.\n",
    " \n",
    "Previously we talked about splitting the data into training and test sets.\n",
    "In statistics, this is not common, and the models are trained with all the data.\n",
    "This is because statistics is generally more interested in the effect of a particular variable *across the entire dataset* than it is about using that variable to make a prediction about a particular datapoint.\n",
    "Because of this, we typically have concerns about how well linear regression will work with new data, i.e. will it have the same $r^2$ for new data or a lower $r^2$?\n",
    "Both forward and backward selection potentially enhance this problem because they tune the model to the data even more closely by removing variables that aren't \"important.\"\n",
    "You should always be very careful with such variable selection methods and their implications for model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "In the birth weight example, there is also information available about the mother's activity level during her pregnancy.\n",
    " Values for this categorical variable are: low, moderate, and high.\n",
    " How can we incorporate these into the model? \n",
    " Since they are not numeric, we have to create *dummy variables*  that are numeric to use.\n",
    " A dummy variable represents the presence or absence of a level of the categorical variable by a 1 and the absence by a zero.\n",
    "  Fortunately, most software packages that do multiple regression do this for us automatically.\n",
    " \n",
    "Often, one of the levels of the categorical variable is considered the \"baseline\" and the contributions to the response of the other levels are in relation to baseline.\n",
    "Let's look at the data again. \n",
    " In the table below, the mother's age is dropped and the mother's activity level (MAL) is included.\n",
    " \n",
    " | Coefficients | Estimate | Std. Error | p-value  |\n",
    "|--------------|----------|------------|----------|\n",
    "| Intercept    | 31.35    | 4.65       | 3.68e-07 |\n",
    "| MW           | 2.74     | 0.82       | 0.0026   |\n",
    "| MH           | -0.04    | 0.02       | 0.0420   |\n",
    "| GP           | 1.11     | 1.03       | 0.2917   |\n",
    "| MALmoderate  | -2.97    | 1.44       | 0.049     |\n",
    "| MALhigh      | -1.45    | 2.69       | 0.5946   |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variable MAL,  MAL low has been chosen as the base line.\n",
    " The other two levels have parameter estimates that we can use to determine which are significantly different from the low level.\n",
    " This makes sense because all mothers will at least have low activity level, and the two additional dummy variables `MALhigh` and `MALmoderate` just get added on top of that.\n",
    " \n",
    " We can see that MAL moderate level is significantly different from the low level (p-value < 0.05).\n",
    " The parameter estimate for the moderate level of MAL is -2.97.\n",
    " This can be interpreted as: being in the moderately active group decreases birth weight by 2.97 units compared to babies in the low activity group.\n",
    " We also see that for babies with mothers in the high activity group, their birth weights are not different from birth weights in the low group, since the p-value is not low (0.5946 &gt; 0.05) and so this term does not have a significant effect on the response (birth weight).\n",
    " \n",
    "  This example highlights a phenomenon that often happens in multiple regression.\n",
    " When we drop the variable MA (mother's age) from the model and the categorical variable is included, both MW (mother's weight) and MH (mother's height) are both important predictors of birth weight (p-values 0.0026 and 0.0420 respectively).\n",
    " This is why it is important to perform some systematic model selection (forward or backward or all possible) to find an optimum set of features.\n",
    " \n",
    "# Diagnostics\n",
    "\n",
    "As in the simple linear regression case, we can use the residuals to check the fit of the model.\n",
    " Recall that the residuals are the observed response minus the predicted response.\n",
    " \n",
    "  - Plot the residuals against each independent variable to check whether higher order terms are needed  \n",
    "  - Plot the residuals versus the predicted values to check whether the     variance is constant  \n",
    "  - Plot a qq-plot of the residuals to check for normality  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity\n",
    "\n",
    "Multicollinearity occurs when two variables or features are linearly related, i.e.\n",
    " they have very strong correlation between them (close to -1 or 1).\n",
    " Practically this means that some of the independent variables are measuring the same thing and are not needed.\n",
    " In the extreme case (close to -1 or 1), the estimates of the parameters of the model cannot be obtained.\n",
    " This is because there is no unique solution for OLS when multicolinearity occurs.\n",
    " As a result, multicollinearity makes conclusions about which features should be used questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
